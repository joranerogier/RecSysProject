{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loving-vehicle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required directory 'C:\\Users\\Jorane Rogier\\Documents\\studie\\year2\\Research Internship\\RecSysProject/output/' exists \n",
      "Required directory 'C:\\Users\\Jorane Rogier\\Documents\\studie\\year2\\Research Internship\\RecSysProject/output//synthetic_data/' exists \n",
      "Required directory 'C:\\Users\\Jorane Rogier\\Documents\\studie\\year2\\Research Internship\\RecSysProject/output//partitioned_data/' exists \n"
     ]
    }
   ],
   "source": [
    "from lenskit.datasets import ML100K\n",
    "from transform_data_representation import transform_dense_to_sparse_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "difficult-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "ml100k = ML100K('ml-100k')\n",
    "ratings = ml100k.ratings\n",
    "ratings = ratings[['user', 'item', 'rating']]\n",
    "user_item_matrix = ratings.pivot(*ratings.columns)\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "user_item_matrix.columns = user_item_matrix.columns.astype(str)\n",
    "df = pd.DataFrame(user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sustained-campus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "# Create Global Item Ranking (GIR), looping over all items, count how often the item is watched\n",
    "GIR = {}\n",
    "for col in df:\n",
    "    # does not take into account movies that have not been watched\n",
    "    summed_rating = df[col].sum()\n",
    "\n",
    "    # only take into account items that have been rated more than 5 times\n",
    "    if (df[col].astype(bool).sum(axis=0) > 100):\n",
    "        GIR[col] = summed_rating\n",
    "\n",
    "ranks_GIR = {k: v for k,v in sorted(GIR.items(), key=lambda item: item[1], reverse=True)}\n",
    "ranks_GIR_items = [*ranks_GIR]\n",
    "print(len(ranks_GIR_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "regional-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user mainstreaminess (UM)\n",
    "user_mainstreamnesses = {}\n",
    "taus = []\n",
    "for uid in range(len(df)):\n",
    "    user_dict = {}\n",
    "    for item_id in df.columns:\n",
    "        rating = df.iloc[uid][item_id]\n",
    "        # only take into account items that have been rated > 5 times\n",
    "        if item_id in GIR:\n",
    "            user_dict[item_id] = rating\n",
    "    ranks_user = {k: v for k,v in sorted(user_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    ranks_user_items = [*ranks_user]\n",
    "    # compute the mainstreaminess measure with Kendall's rank-order correlation\n",
    "    tau, p_value = stats.kendalltau(ranks_GIR_items, ranks_user_items)\n",
    "    user_mainstreamnesses[uid] = [tau, p_value]\n",
    "    taus.append(round(tau, 3))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-expense",
   "metadata": {},
   "source": [
    "### Divide users into two groups, based on mainstreaminess score, and compare group-size characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-illinois",
   "metadata": {},
   "source": [
    "First test with cut-off points at tau = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "english-indicator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n",
      "670\n"
     ]
    }
   ],
   "source": [
    "# Get the user-ids which have tau < 0.02 (beyond mainstream users, bmu)\n",
    "bmu = [k for k, v in user_mainstreamnesses.items() if float(v[0]) < 0.02]\n",
    "print(len(bmu))\n",
    "\n",
    "# Get the user-ids which have tau >= 0.02 (mainstream users, mu)\n",
    "mu = [k for k, v in user_mainstreamnesses.items() if float(v[0]) >= 0.02]\n",
    "print(len(mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afraid-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmu = df.iloc[bmu]\n",
    "df_mu = df.iloc[mu]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-warren",
   "metadata": {},
   "source": [
    "Then, compare the characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "departmental-uganda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-64cac0790160>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bmu['total'] = df.gt(0).sum(axis=1)\n",
      "<ipython-input-42-64cac0790160>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mu['total'] = df.gt(0).sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# compute non-zero values per row for both dataframes\n",
    "df_bmu['total'] = df.gt(0).sum(axis=1)\n",
    "df_mu['total'] = df.gt(0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "exterior-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "685\n",
      "97.27611940298507\n"
     ]
    }
   ],
   "source": [
    "print(df_mu['total'].min())\n",
    "print(df_mu['total'].max())\n",
    "print(df_mu['total'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "approximate-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "737\n",
      "127.56410256410257\n"
     ]
    }
   ],
   "source": [
    "print(df_bmu['total'].min())\n",
    "print(df_bmu['total'].max())\n",
    "print(df_bmu['total'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "mexican-doctrine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Characteristic  Beyond-mainstream  Mainstream\n",
      "0  Min. #Ratings          20.000000   20.000000\n",
      "1  Max. #Ratings         685.000000  737.000000\n",
      "2  Mean #Ratings          97.276119  127.564103\n"
     ]
    }
   ],
   "source": [
    "data = {'Characteristic': ['Min. #Ratings', 'Max. #Ratings', 'Mean #Ratings'],\n",
    "                'Beyond-mainstream': [df_mu['total'].min(), df_mu['total'].max(), df_mu['total'].mean()],\n",
    "                'Mainstream': [df_bmu['total'].min(), df_bmu['total'].max(), df_bmu['total'].mean()]}\n",
    "\n",
    "compare_df = pd.DataFrame(data)\n",
    "print(compare_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
